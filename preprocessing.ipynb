{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c89c9bb",
   "metadata": {},
   "source": [
    "# Before starting preprocessing, please complete the following steps:\n",
    "\n",
    "# Category 2 / XA2017010006426\n",
    "#    - This image requires manual re-grabbing.\n",
    "\n",
    "# Category 3 / XA2017080014862\n",
    "#    - This patient has a fixture; it is recommended to discard the images.\n",
    "\n",
    "# Category 3 / XA2018070021907\n",
    "#    - This image requires manual re-grabbing.\n",
    "\n",
    "# Category 1/ XA2017030007058\n",
    "#    - This patient has a fixture; it is recommended to discard the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f6c0a",
   "metadata": {},
   "source": [
    "# clean incomplete patient\n",
    "# e.g. image number != 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67b7ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🩻 Patient integrity cleaning complete.\n",
      "✅ Kept folders   : 3063\n",
      "🗑️  Removed folders: 0 / 3063 total\n",
      "🎉 All folders are complete (4 images each).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def clean_incomplete_patients(root_dir, expected_num=4):\n",
    "    removed = []\n",
    "    kept = 0\n",
    "    total = 0\n",
    "\n",
    "    for category in sorted(os.listdir(root_dir)):\n",
    "        cat_path = os.path.join(root_dir, category)\n",
    "        if not os.path.isdir(cat_path):\n",
    "            continue\n",
    "\n",
    "        for patient in sorted(os.listdir(cat_path)):\n",
    "            p_path = os.path.join(cat_path, patient)\n",
    "            if not os.path.isdir(p_path):\n",
    "                continue\n",
    "\n",
    "            imgs = [f for f in os.listdir(p_path)\n",
    "                    if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp'))]\n",
    "            total += 1\n",
    "\n",
    "            if len(imgs) < expected_num:\n",
    "                removed.append((category, patient, len(imgs)))\n",
    "                shutil.rmtree(p_path) # remove incomplete patient folder\n",
    "            if len(imgs) > expected_num:\n",
    "                print(f\"⚠️  Warning: Patient {patient} in category {category} has {len(imgs)} images (expected {expected_num}).\")\n",
    "                print(\"    Please check manually.\")\n",
    "                kept += 1\n",
    "            else:\n",
    "                kept += 1\n",
    "\n",
    "    print(\"🩻 Patient integrity cleaning complete.\")\n",
    "    print(f\"✅ Kept folders   : {kept}\")\n",
    "    print(f\"🗑️  Removed folders: {len(removed)} / {total} total\")\n",
    "\n",
    "    if removed:\n",
    "        print(\"\\nExamples of removed folders:\")\n",
    "        for i, (cat, pid, n) in enumerate(removed[:10]):\n",
    "            print(f\" {i+1}. [{cat}] {pid} → {n} images (removed)\")\n",
    "    else:\n",
    "        print(\"🎉 All folders are complete (4 images each).\")\n",
    "\n",
    "# 使用範例\n",
    "clean_incomplete_patients(\"check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6677cc",
   "metadata": {},
   "source": [
    "# rename as L-CC,R-CC,L-MLO,R-MLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c153bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0 影像已輸出至新資料夾。\n",
      "Category 1 影像已輸出至新資料夾。\n",
      "Category 2 影像已輸出至新資料夾。\n",
      "Category 3 影像已輸出至新資料夾。\n",
      "Category 4 影像已輸出至新資料夾。\n",
      "Category 5 影像已輸出至新資料夾。\n",
      "Category 6 影像已輸出至新資料夾。\n",
      "\n",
      "🎯 所有影像已依規則重新命名並輸出完成！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def judge_left_right(filepath, threshold=30):\n",
    "    \"\"\"\n",
    "    使用像素比例判斷左右（L/R）\n",
    "    Args:\n",
    "        filepath: 影像路徑\n",
    "        threshold: 像素亮度閾值，預設30\n",
    "    Returns:\n",
    "        side: 'L' 或 'R'\n",
    "    \"\"\"\n",
    "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    h, w = img.shape\n",
    "    left_half = img[:, :w // 2]\n",
    "    right_half = img[:, w // 2:]\n",
    "\n",
    "    left_ratio = np.sum(left_half > threshold)\n",
    "    right_ratio = np.sum(right_half > threshold)\n",
    "    if left_ratio == 0 and right_ratio == 0:\n",
    "        return None\n",
    "    if left_ratio > right_ratio:\n",
    "        return \"L\"\n",
    "    elif right_ratio > left_ratio:\n",
    "        return \"R\"\n",
    "    # 若相等則以較大亮度區域判斷\n",
    "    left_bright = np.sum(left_half[left_half > threshold])\n",
    "    right_bright = np.sum(right_half[right_half > threshold])\n",
    "    if left_bright >= right_bright:\n",
    "        side = \"L\"\n",
    "    else:\n",
    "        side = \"R\"\n",
    "    return side\n",
    "\n",
    "\n",
    "def extract_order_from_filename(filename):\n",
    "    \"\"\"\n",
    "    從檔名中提取最後的數字，作為排序依據。\n",
    "    e.g. \"image-3.png\" → 3\n",
    "    \"\"\"\n",
    "    name, _ = os.path.splitext(filename)\n",
    "    parts = name.split('-')\n",
    "    for part in reversed(parts):\n",
    "        if part.isdigit():\n",
    "            return int(part)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def rename_mammogram_images_by_order(root_dir, output_root_dir):\n",
    "    \"\"\"\n",
    "    依據左右判斷與檔名尾數順序，\n",
    "    將影像命名為 L-CC / L-MLO / R-CC / R-MLO，\n",
    "    並輸出到新的資料夾結構中。\n",
    "    \"\"\"\n",
    "    os.makedirs(output_root_dir, exist_ok=True)\n",
    "\n",
    "    for category in sorted(os.listdir(root_dir)):\n",
    "        category_path = os.path.join(root_dir, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "\n",
    "        # 建立輸出類別資料夾\n",
    "        output_category_path = os.path.join(output_root_dir, category)\n",
    "        os.makedirs(output_category_path, exist_ok=True)\n",
    "\n",
    "        for patient_id in sorted(os.listdir(category_path)):\n",
    "            patient_path = os.path.join(category_path, patient_id)\n",
    "            if not os.path.isdir(patient_path):\n",
    "                continue\n",
    "\n",
    "            output_patient_path = os.path.join(output_category_path, patient_id)\n",
    "            os.makedirs(output_patient_path, exist_ok=True)\n",
    "\n",
    "            # 找出該病人資料夾下所有影像\n",
    "            image_files = [\n",
    "                f for f in os.listdir(patient_path)\n",
    "                if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "            ]\n",
    "            if not image_files:\n",
    "                continue\n",
    "\n",
    "            # 先依左右分類\n",
    "            left_images, right_images = [], []\n",
    "            for file in image_files:\n",
    "                full_path = os.path.join(patient_path, file)\n",
    "                side = judge_left_right(full_path)\n",
    "                if side == \"L\":\n",
    "                    left_images.append(file)\n",
    "                elif side == \"R\":\n",
    "                    right_images.append(file)\n",
    "                else:\n",
    "                    print(f\"⚠️ 無法判斷左右：{file}\")\n",
    "\n",
    "            # 重新命名後複製\n",
    "            def rename_and_copy(img_list, side):\n",
    "                if not img_list:\n",
    "                    return\n",
    "                sorted_imgs = sorted(img_list, key=extract_order_from_filename)\n",
    "                for i, file in enumerate(sorted_imgs):\n",
    "                    label = \"CC\" if i == 0 else \"MLO\"\n",
    "                    _, ext = os.path.splitext(file)\n",
    "                    new_name = f\"{side}-{label}{ext}\"\n",
    "                    old_path = os.path.join(patient_path, file)\n",
    "                    new_path = os.path.join(output_patient_path, new_name)\n",
    "                    shutil.copy2(old_path, new_path)\n",
    "\n",
    "            rename_and_copy(left_images, \"L\")\n",
    "            rename_and_copy(right_images, \"R\")\n",
    "\n",
    "        print(f\"{category} 影像已輸出至新資料夾。\")\n",
    "\n",
    "    print(\"\\n🎯 所有影像已依規則重新命名並輸出完成！\")\n",
    "\n",
    "\n",
    "# 🚀 使用範例\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"check\"\n",
    "    output_root_dir = \"datasets\"\n",
    "    rename_mammogram_images_by_order(root_dir, output_root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03ab3e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2294, 1914, 3)\n"
     ]
    }
   ],
   "source": [
    "# print img size\n",
    "# pip install opencv-python\n",
    "import cv2\n",
    "\n",
    "img_path = \"datasets/Category 1/XA2015110006335/R-MLO.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "print(img.shape)  # (height, width, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f12bb0",
   "metadata": {},
   "source": [
    "# spilt datasets and make annotation for trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eac977bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1197 descriptions from datasets/Catagory 0.csv\n",
      "✅ Loaded 499 descriptions from datasets/Catagory 1.csv\n",
      "✅ Loaded 499 descriptions from datasets/Catagory 2.csv\n",
      "✅ Loaded 499 descriptions from datasets/Catagory 3.csv\n",
      "✅ Loaded 262 descriptions from datasets/Catagory 4.csv\n",
      "✅ Loaded 38 descriptions from datasets/Catagory 5.csv\n",
      "✅ Loaded 69 descriptions from datasets/Catagory 6.csv\n",
      "\n",
      "📂 Scanning dataset folder: datasets\n",
      "⏭️ Skip Category 6\n",
      "\n",
      "✅ Output complete: 2992 patients\n",
      "  Train: 2094 | Val: 300 | Test: 598\n",
      "📁 Output folder: datasets\n",
      "\n",
      "📊 Dataset counts per class (stratified with rounding):\n",
      "  Category 0 (Total: 1195):\n",
      "    Train=836 (70.0%), Val=120 (10.0%), Test=239 (20.0%)\n",
      "  Category 1 (Total: 499):\n",
      "    Train=349 (69.9%), Val=50 (10.0%), Test=100 (20.0%)\n",
      "  Category 2 (Total: 500):\n",
      "    Train=350 (70.0%), Val=50 (10.0%), Test=100 (20.0%)\n",
      "  Category 3 (Total: 498):\n",
      "    Train=349 (70.1%), Val=50 (10.0%), Test=99 (19.9%)\n",
      "  Category 4 (Total: 261):\n",
      "    Train=183 (70.1%), Val=26 (10.0%), Test=52 (19.9%)\n",
      "  Category 5 (Total: 39):\n",
      "    Train=27 (69.2%), Val=4 (10.3%), Test=8 (20.5%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "\n",
    "VALID_VIEWS = ['L-CC', 'R-CC', 'L-MLO', 'R-MLO']\n",
    "\n",
    "def load_patient_descriptions(base_datasets_path):\n",
    "    descriptions = {}\n",
    "    for category_num in range(7):\n",
    "        csv_path = os.path.join(base_datasets_path, f\"Catagory {category_num}.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"⚠️ CSV not found: {csv_path}\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            for _, row in df.iterrows():\n",
    "                patient_id = str(row.iloc[0])\n",
    "                description = str(row.iloc[-1]) if len(row) > 1 else \"No description\"\n",
    "                descriptions[patient_id] = description\n",
    "            print(f\"✅ Loaded {len(df)} descriptions from {csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to load {csv_path}: {e}\")\n",
    "    return descriptions\n",
    "\n",
    "\n",
    "def stratified_split(df, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    改進版分層抽樣：使用四捨五入避免截斷誤差\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    train_list, val_list, test_list = [], [], []\n",
    "\n",
    "    for label in sorted(df['label'].unique()):\n",
    "        df_label = df[df['label'] == label].sample(frac=1, random_state=seed)\n",
    "        n_total = len(df_label)\n",
    "        \n",
    "        # 使用四捨五入計算各集合大小\n",
    "        n_train = max(1, round(n_total * train_ratio))\n",
    "        n_val = max(1, round(n_total * val_ratio))\n",
    "        \n",
    "        # 確保總數正確：test 集合吸收所有誤差\n",
    "        n_test = max(1, n_total - n_train - n_val)\n",
    "        \n",
    "        # 處理邊界情況：總和超過 n_total\n",
    "        if n_train + n_val + n_test > n_total:\n",
    "            excess = n_train + n_val + n_test - n_total\n",
    "            # 優先從最大的集合減少\n",
    "            if n_train >= n_val and n_train > excess:\n",
    "                n_train -= excess\n",
    "            elif n_val > excess:\n",
    "                n_val -= excess\n",
    "            else:\n",
    "                n_test -= excess\n",
    "        \n",
    "        # 處理邊界情況：總和小於 n_total（理論上不應發生）\n",
    "        elif n_train + n_val + n_test < n_total:\n",
    "            n_test += (n_total - (n_train + n_val + n_test))\n",
    "        \n",
    "        # 確保每個集合至少有 1 筆資料（如果類別總數 >= 3）\n",
    "        if n_total >= 3:\n",
    "            n_train = max(1, n_train)\n",
    "            n_val = max(1, n_val)\n",
    "            n_test = max(1, n_test)\n",
    "        \n",
    "        train_list.append(df_label.iloc[:n_train])\n",
    "        val_list.append(df_label.iloc[n_train:n_train+n_val])\n",
    "        test_list.append(df_label.iloc[n_train+n_val:n_train+n_val+n_test])\n",
    "\n",
    "    train_df = pd.concat(train_list).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    val_df = pd.concat(val_list).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_list).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def generate_multiview_csvs(base_dir, base_datasets_path, output_dir,\n",
    "                            train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, seed=42):\n",
    "    random.seed(seed)\n",
    "    patient_descriptions = load_patient_descriptions(base_datasets_path)\n",
    "    patients_data = []\n",
    "\n",
    "    print(f\"\\n📂 Scanning dataset folder: {base_dir}\")\n",
    "    \n",
    "    for category in sorted(os.listdir(base_dir)):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        try:\n",
    "            label = int(category.replace(\"Category \", \"\"))\n",
    "            if label == 6:\n",
    "                print(f\"⏭️ Skip Category {label}\")\n",
    "                continue\n",
    "        except ValueError:\n",
    "            print(f\"⚠️ Skip unrecognized folder name '{category}'\")\n",
    "            continue\n",
    "\n",
    "        for patient_folder in os.listdir(category_path):\n",
    "            patient_path = os.path.join(category_path, patient_folder)\n",
    "            if not os.path.isdir(patient_path):\n",
    "                continue\n",
    "\n",
    "            patient_entry = {\n",
    "                'patient_id': f\"{category}/{patient_folder}\",\n",
    "                'label': label,\n",
    "                'description': patient_descriptions.get(patient_folder, \"No description available\")\n",
    "            }\n",
    "\n",
    "            for img_file in os.listdir(patient_path):\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    view_name = os.path.splitext(img_file)[0]\n",
    "                    if view_name in VALID_VIEWS:\n",
    "                        patient_entry[view_name] = os.path.join(category, patient_folder, img_file)\n",
    "\n",
    "            patients_data.append(patient_entry)\n",
    "\n",
    "    df = pd.DataFrame(patients_data)\n",
    "    df = df.reindex(columns=VALID_VIEWS + ['label', 'description', 'patient_id'])\n",
    "\n",
    "    missing_mask = df[VALID_VIEWS].isna().any(axis=1)\n",
    "    if missing_mask.any():\n",
    "        print(\"\\n⚠️ Patients missing some views:\")\n",
    "        for _, row in df[missing_mask].iterrows():\n",
    "            missing_views = [v for v in VALID_VIEWS if pd.isna(row[v])]\n",
    "            print(f\"  - {row['patient_id']} missing {', '.join(missing_views)}\")\n",
    "\n",
    "    df = df.dropna(subset=VALID_VIEWS)\n",
    "\n",
    "    # 分層抽樣（使用改進版）\n",
    "    train_df, val_df, test_df = stratified_split(df, train_ratio, val_ratio, test_ratio, seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_df.to_csv(os.path.join(output_dir, \"train_labels.csv\"), index=False)\n",
    "    val_df.to_csv(os.path.join(output_dir, \"val_labels.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(output_dir, \"test_labels.csv\"), index=False)\n",
    "\n",
    "    print(f\"\\n✅ Output complete: {len(df)} patients\")\n",
    "    print(f\"  Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "    print(f\"📁 Output folder: {output_dir}\")\n",
    "\n",
    "    # 每個類別統計（包含實際比例）\n",
    "    print(\"\\n📊 Dataset counts per class (stratified with rounding):\")\n",
    "    for label in sorted(df['label'].unique()):\n",
    "        n_total_c = (df['label'] == label).sum()\n",
    "        n_train_c = (train_df['label'] == label).sum()\n",
    "        n_val_c   = (val_df['label'] == label).sum()\n",
    "        n_test_c  = (test_df['label'] == label).sum()\n",
    "        \n",
    "        actual_train_ratio = n_train_c / n_total_c if n_total_c > 0 else 0\n",
    "        actual_val_ratio = n_val_c / n_total_c if n_total_c > 0 else 0\n",
    "        actual_test_ratio = n_test_c / n_total_c if n_total_c > 0 else 0\n",
    "        \n",
    "        print(f\"  Category {label} (Total: {n_total_c}):\")\n",
    "        print(f\"    Train={n_train_c} ({actual_train_ratio:.1%}), \"\n",
    "              f\"Val={n_val_c} ({actual_val_ratio:.1%}), \"\n",
    "              f\"Test={n_test_c} ({actual_test_ratio:.1%})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"datasets\"\n",
    "    base_datasets_path = base_dir\n",
    "    output_dir = base_dir\n",
    "\n",
    "    generate_multiview_csvs(\n",
    "        base_dir=base_dir,\n",
    "        base_datasets_path=base_datasets_path,\n",
    "        output_dir=output_dir,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.2,\n",
    "        seed=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7bf291",
   "metadata": {},
   "source": [
    "# clahe \n",
    "# crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c69b19a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始影像尺寸: (2294, 1914)\n",
      "裁切後影像尺寸: (2142, 793)\n",
      "有效像素數: 835177\n",
      "遮罩覆蓋率: 19.02%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def preprocess_and_autocrop(image_path, threshold_offset=-50, padding_percent=0.08, min_area_ratio=0.01):\n",
    "    \"\"\"\n",
    "    預處理並自動裁切乳房X光影像\n",
    "    \n",
    "    參數:\n",
    "        image_path: 輸入影像路徑\n",
    "        threshold_offset: 閾值偏移量（負值=更寬鬆，預設-20）\n",
    "        padding_percent: 裁切邊距百分比（預設0.05=5%）\n",
    "        min_area_ratio: 最小輪廓面積比例（預設0.02=2%）\n",
    "    \n",
    "    返回:\n",
    "        original: 原始灰階影像\n",
    "        enhanced: CLAHE 增強後的影像\n",
    "        mask: 偵測到的乳房遮罩\n",
    "        cropped: 自動裁切後的影像\n",
    "    \"\"\"\n",
    "    \n",
    "    # 讀取影像\n",
    "    original = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if original is None:\n",
    "        raise ValueError(f\"無法讀取影像: {image_path}\")\n",
    "    \n",
    "    # 1. CLAHE 對比度增強\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(original)\n",
    "    \n",
    "    # 2. 影像預處理\n",
    "    blurred = cv2.GaussianBlur(enhanced, (5, 5), 0)\n",
    "    \n",
    "    # 3. 二值化 - 使用更低的閾值來包含暗區（如乳頭）\n",
    "    # 先用 Otsu 得到基準閾值\n",
    "    otsu_thresh, _ = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    # 大幅降低閾值以包含乳頭等暗區\n",
    "    lower_thresh = max(5, otsu_thresh + threshold_offset)\n",
    "    _, binary = cv2.threshold(blurred, lower_thresh, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # 4. 形態學處理 - 更激進的閉運算\n",
    "    kernel_large = np.ones((11, 11), np.uint8)\n",
    "    kernel_small = np.ones((5, 5), np.uint8)\n",
    "    \n",
    "    # 更多次閉運算，填補更多區域\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel_large, iterations=5)\n",
    "    # 減少開運算次數，保留更多區域\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_small, iterations=1)\n",
    "    \n",
    "    # 5. 尋找輪廓\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # 6. 過濾並選擇最大輪廓 - 降低最小面積要求\n",
    "    min_area = original.shape[0] * original.shape[1] * min_area_ratio\n",
    "    valid_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
    "    \n",
    "    if len(valid_contours) == 0:\n",
    "        raise ValueError(\"未找到有效輪廓\")\n",
    "    \n",
    "    largest_contour = max(valid_contours, key=cv2.contourArea)\n",
    "    \n",
    "    # 7. 計算凸包，包含更完整的區域\n",
    "    hull = cv2.convexHull(largest_contour)\n",
    "    \n",
    "    # 8. 創建遮罩 - 使用凸包確保包含完整乳房區域\n",
    "    mask = np.zeros_like(original)\n",
    "    cv2.drawContours(mask, [hull], -1, 255, thickness=cv2.FILLED)\n",
    "    \n",
    "    # 9. 額外處理：填補內部孔洞（如乳頭區域）\n",
    "    # 尋找所有內部輪廓\n",
    "    contours_all, hierarchy = cv2.findContours(mask.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # 填補所有內部孔洞\n",
    "    for i in range(len(contours_all)):\n",
    "        if hierarchy[0][i][3] != -1:  # 如果有父輪廓（即內部孔洞）\n",
    "            cv2.drawContours(mask, contours_all, i, 255, thickness=cv2.FILLED)\n",
    "    \n",
    "    # 10. 擴張遮罩邊緣（更溫和的擴張以保留細節）\n",
    "    kernel_dilate = np.ones((15, 15), np.uint8)\n",
    "    mask = cv2.dilate(mask, kernel_dilate, iterations=3)\n",
    "    \n",
    "    # 10. 羽化邊緣\n",
    "    mask_blurred = cv2.GaussianBlur(mask, (21, 21), 0)\n",
    "    \n",
    "    # 11. 獲取邊界框進行裁切\n",
    "    x, y, w, h = cv2.boundingRect(hull)\n",
    "    \n",
    "    # 添加更大的邊距（基於影像尺寸的百分比）\n",
    "    padding_x = int(original.shape[1] * padding_percent)\n",
    "    padding_y = int(original.shape[0] * padding_percent)\n",
    "    \n",
    "    x = max(0, x - padding_x)\n",
    "    y = max(0, y - padding_y)\n",
    "    w = min(original.shape[1] - x, w + 2 * padding_x)\n",
    "    h = min(original.shape[0] - y, h + 2 * padding_y)\n",
    "    \n",
    "    # 12. 裁切影像和遮罩\n",
    "    cropped = enhanced[y:y+h, x:x+w]\n",
    "\n",
    "    return original, enhanced, mask, cropped\n",
    "\n",
    "\n",
    "def display_results(original, enhanced, mask, cropped):\n",
    "    \"\"\"\n",
    "    顯示處理結果\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(original, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.title(\"After CLAHE\")\n",
    "    plt.imshow(enhanced, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.title(\"Detected Mask\")\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.title(\"Auto-cropped Image\")\n",
    "    plt.imshow(cropped, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === 使用範例 ===\n",
    "if __name__ == \"__main__\":\n",
    "    # === 測試 ===\n",
    "    image_path = \"datasets/Category 0/XA2016010003906/R-CC.jpg\"\n",
    "    \n",
    "    try:    \n",
    "        original, enhanced, mask, cropped = preprocess_and_autocrop(\n",
    "            image_path,\n",
    "            threshold_offset=-50,    # 更寬鬆的閾值（預設-20）\n",
    "            padding_percent=0.08,    # 更大的邊距（預設0.05=5%）\n",
    "            min_area_ratio=0.01      # 更低的最小面積（預設0.02=2%）\n",
    "        )\n",
    "        \n",
    "        #display_results(original, enhanced, mask, cropped)\n",
    "        \n",
    "        # 輸出統計資訊\n",
    "        print(f\"原始影像尺寸: {original.shape}\")\n",
    "        print(f\"裁切後影像尺寸: {cropped.shape}\")\n",
    "        print(f\"有效像素數: {np.sum(mask > 0)}\")\n",
    "        print(f\"遮罩覆蓋率: {np.sum(mask > 0) / (mask.shape[0] * mask.shape[1]) * 100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df367bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def save_cropped_images(cropped, image_path, output_root):\n",
    "    \"\"\"\n",
    "    保存裁切後的影像，並維持與原始資料夾相同的結構。\n",
    "    \"\"\"\n",
    "    relative_path = os.path.relpath(image_path, start=\"datasets\")\n",
    "    output_dir = os.path.join(output_root, os.path.dirname(relative_path))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{base_name}.jpg\"), cropped)\n",
    "\n",
    "def process_dataset(input_root, output_root):\n",
    "    \"\"\"\n",
    "    遍歷資料夾中的所有影像，進行預處理並保存結果。\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(input_root):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tiff')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    original, enhanced, mask, cropped = preprocess_and_autocrop(image_path)\n",
    "                    save_cropped_images(cropped, image_path, output_root)\n",
    "                    #print(f\"處理完成: {image_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"處理失敗: {image_path}, 錯誤: {e}\")\n",
    "\n",
    "# === 主程式 ===\n",
    "if __name__ == \"__main__\":\n",
    "    input_root = \"datasets\"  # 原始資料夾\n",
    "    output_root = \"cropped_datasets\"  # 預處理後的資料夾\n",
    "    process_dataset(input_root, output_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pacs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

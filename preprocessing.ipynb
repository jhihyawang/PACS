{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c89c9bb",
   "metadata": {},
   "source": [
    "# Before starting preprocessing, please complete the following steps:\n",
    "\n",
    "# Category 2 / XA2017010006426\n",
    "#    - This image requires manual re-grabbing.\n",
    "\n",
    "# Category 3 / XA2017080014862\n",
    "#    - This patient has a fixture; it is recommended to discard the images.\n",
    "\n",
    "# Category 3 / XA2018070021907\n",
    "#    - This image requires manual re-grabbing.\n",
    "\n",
    "# Category 1/ XA2017030007058\n",
    "#    - This patient has a fixture; it is recommended to discard the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f6c0a",
   "metadata": {},
   "source": [
    "# clean incomplete patient\n",
    "# e.g. image number != 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67b7ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ©» Patient integrity cleaning complete.\n",
      "âœ… Kept folders   : 3063\n",
      "ğŸ—‘ï¸  Removed folders: 0 / 3063 total\n",
      "ğŸ‰ All folders are complete (4 images each).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def clean_incomplete_patients(root_dir, expected_num=4):\n",
    "    removed = []\n",
    "    kept = 0\n",
    "    total = 0\n",
    "\n",
    "    for category in sorted(os.listdir(root_dir)):\n",
    "        cat_path = os.path.join(root_dir, category)\n",
    "        if not os.path.isdir(cat_path):\n",
    "            continue\n",
    "\n",
    "        for patient in sorted(os.listdir(cat_path)):\n",
    "            p_path = os.path.join(cat_path, patient)\n",
    "            if not os.path.isdir(p_path):\n",
    "                continue\n",
    "\n",
    "            imgs = [f for f in os.listdir(p_path)\n",
    "                    if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.bmp'))]\n",
    "            total += 1\n",
    "\n",
    "            if len(imgs) < expected_num:\n",
    "                removed.append((category, patient, len(imgs)))\n",
    "                shutil.rmtree(p_path) # remove incomplete patient folder\n",
    "            if len(imgs) > expected_num:\n",
    "                print(f\"âš ï¸  Warning: Patient {patient} in category {category} has {len(imgs)} images (expected {expected_num}).\")\n",
    "                print(\"    Please check manually.\")\n",
    "                kept += 1\n",
    "            else:\n",
    "                kept += 1\n",
    "\n",
    "    print(\"ğŸ©» Patient integrity cleaning complete.\")\n",
    "    print(f\"âœ… Kept folders   : {kept}\")\n",
    "    print(f\"ğŸ—‘ï¸  Removed folders: {len(removed)} / {total} total\")\n",
    "\n",
    "    if removed:\n",
    "        print(\"\\nExamples of removed folders:\")\n",
    "        for i, (cat, pid, n) in enumerate(removed[:10]):\n",
    "            print(f\" {i+1}. [{cat}] {pid} â†’ {n} images (removed)\")\n",
    "    else:\n",
    "        print(\"ğŸ‰ All folders are complete (4 images each).\")\n",
    "\n",
    "# ä½¿ç”¨ç¯„ä¾‹\n",
    "clean_incomplete_patients(\"check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6677cc",
   "metadata": {},
   "source": [
    "# rename as L-CC,R-CC,L-MLO,R-MLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c153bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0 å½±åƒå·²è¼¸å‡ºè‡³æ–°è³‡æ–™å¤¾ã€‚\n",
      "Category 1 å½±åƒå·²è¼¸å‡ºè‡³æ–°è³‡æ–™å¤¾ã€‚\n",
      "Category 2 å½±åƒå·²è¼¸å‡ºè‡³æ–°è³‡æ–™å¤¾ã€‚\n",
      "Category 3 å½±åƒå·²è¼¸å‡ºè‡³æ–°è³‡æ–™å¤¾ã€‚\n",
      "Category 4 å½±åƒå·²è¼¸å‡ºè‡³æ–°è³‡æ–™å¤¾ã€‚\n",
      "Category 5 å½±åƒå·²è¼¸å‡ºè‡³æ–°è³‡æ–™å¤¾ã€‚\n",
      "Category 6 å½±åƒå·²è¼¸å‡ºè‡³æ–°è³‡æ–™å¤¾ã€‚\n",
      "\n",
      "ğŸ¯ æ‰€æœ‰å½±åƒå·²ä¾è¦å‰‡é‡æ–°å‘½åä¸¦è¼¸å‡ºå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def judge_left_right(filepath, threshold=30):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨åƒç´ æ¯”ä¾‹åˆ¤æ–·å·¦å³ï¼ˆL/Rï¼‰\n",
    "    Args:\n",
    "        filepath: å½±åƒè·¯å¾‘\n",
    "        threshold: åƒç´ äº®åº¦é–¾å€¼ï¼Œé è¨­30\n",
    "    Returns:\n",
    "        side: 'L' æˆ– 'R'\n",
    "    \"\"\"\n",
    "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    h, w = img.shape\n",
    "    left_half = img[:, :w // 2]\n",
    "    right_half = img[:, w // 2:]\n",
    "\n",
    "    left_ratio = np.sum(left_half > threshold)\n",
    "    right_ratio = np.sum(right_half > threshold)\n",
    "    if left_ratio == 0 and right_ratio == 0:\n",
    "        return None\n",
    "    if left_ratio > right_ratio:\n",
    "        return \"L\"\n",
    "    elif right_ratio > left_ratio:\n",
    "        return \"R\"\n",
    "    # è‹¥ç›¸ç­‰å‰‡ä»¥è¼ƒå¤§äº®åº¦å€åŸŸåˆ¤æ–·\n",
    "    left_bright = np.sum(left_half[left_half > threshold])\n",
    "    right_bright = np.sum(right_half[right_half > threshold])\n",
    "    if left_bright >= right_bright:\n",
    "        side = \"L\"\n",
    "    else:\n",
    "        side = \"R\"\n",
    "    return side\n",
    "\n",
    "\n",
    "def extract_order_from_filename(filename):\n",
    "    \"\"\"\n",
    "    å¾æª”åä¸­æå–æœ€å¾Œçš„æ•¸å­—ï¼Œä½œç‚ºæ’åºä¾æ“šã€‚\n",
    "    e.g. \"image-3.png\" â†’ 3\n",
    "    \"\"\"\n",
    "    name, _ = os.path.splitext(filename)\n",
    "    parts = name.split('-')\n",
    "    for part in reversed(parts):\n",
    "        if part.isdigit():\n",
    "            return int(part)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def rename_mammogram_images_by_order(root_dir, output_root_dir):\n",
    "    \"\"\"\n",
    "    ä¾æ“šå·¦å³åˆ¤æ–·èˆ‡æª”åå°¾æ•¸é †åºï¼Œ\n",
    "    å°‡å½±åƒå‘½åç‚º L-CC / L-MLO / R-CC / R-MLOï¼Œ\n",
    "    ä¸¦è¼¸å‡ºåˆ°æ–°çš„è³‡æ–™å¤¾çµæ§‹ä¸­ã€‚\n",
    "    \"\"\"\n",
    "    os.makedirs(output_root_dir, exist_ok=True)\n",
    "\n",
    "    for category in sorted(os.listdir(root_dir)):\n",
    "        category_path = os.path.join(root_dir, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "\n",
    "        # å»ºç«‹è¼¸å‡ºé¡åˆ¥è³‡æ–™å¤¾\n",
    "        output_category_path = os.path.join(output_root_dir, category)\n",
    "        os.makedirs(output_category_path, exist_ok=True)\n",
    "\n",
    "        for patient_id in sorted(os.listdir(category_path)):\n",
    "            patient_path = os.path.join(category_path, patient_id)\n",
    "            if not os.path.isdir(patient_path):\n",
    "                continue\n",
    "\n",
    "            output_patient_path = os.path.join(output_category_path, patient_id)\n",
    "            os.makedirs(output_patient_path, exist_ok=True)\n",
    "\n",
    "            # æ‰¾å‡ºè©²ç—…äººè³‡æ–™å¤¾ä¸‹æ‰€æœ‰å½±åƒ\n",
    "            image_files = [\n",
    "                f for f in os.listdir(patient_path)\n",
    "                if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "            ]\n",
    "            if not image_files:\n",
    "                continue\n",
    "\n",
    "            # å…ˆä¾å·¦å³åˆ†é¡\n",
    "            left_images, right_images = [], []\n",
    "            for file in image_files:\n",
    "                full_path = os.path.join(patient_path, file)\n",
    "                side = judge_left_right(full_path)\n",
    "                if side == \"L\":\n",
    "                    left_images.append(file)\n",
    "                elif side == \"R\":\n",
    "                    right_images.append(file)\n",
    "                else:\n",
    "                    print(f\"âš ï¸ ç„¡æ³•åˆ¤æ–·å·¦å³ï¼š{file}\")\n",
    "\n",
    "            # é‡æ–°å‘½åå¾Œè¤‡è£½\n",
    "            def rename_and_copy(img_list, side):\n",
    "                if not img_list:\n",
    "                    return\n",
    "                sorted_imgs = sorted(img_list, key=extract_order_from_filename)\n",
    "                for i, file in enumerate(sorted_imgs):\n",
    "                    label = \"CC\" if i == 0 else \"MLO\"\n",
    "                    _, ext = os.path.splitext(file)\n",
    "                    new_name = f\"{side}-{label}{ext}\"\n",
    "                    old_path = os.path.join(patient_path, file)\n",
    "                    new_path = os.path.join(output_patient_path, new_name)\n",
    "                    shutil.copy2(old_path, new_path)\n",
    "\n",
    "            rename_and_copy(left_images, \"L\")\n",
    "            rename_and_copy(right_images, \"R\")\n",
    "\n",
    "        print(f\"{category} å½±åƒå·²è¼¸å‡ºè‡³æ–°è³‡æ–™å¤¾ã€‚\")\n",
    "\n",
    "    print(\"\\nğŸ¯ æ‰€æœ‰å½±åƒå·²ä¾è¦å‰‡é‡æ–°å‘½åä¸¦è¼¸å‡ºå®Œæˆï¼\")\n",
    "\n",
    "\n",
    "# ğŸš€ ä½¿ç”¨ç¯„ä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"check\"\n",
    "    output_root_dir = \"datasets\"\n",
    "    rename_mammogram_images_by_order(root_dir, output_root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03ab3e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2294, 1914, 3)\n"
     ]
    }
   ],
   "source": [
    "# print img size\n",
    "# pip install opencv-python\n",
    "import cv2\n",
    "\n",
    "img_path = \"datasets/Category 1/XA2015110006335/R-MLO.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "print(img.shape)  # (height, width, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f12bb0",
   "metadata": {},
   "source": [
    "# spilt datasets and make annotation for trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eac977bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1197 descriptions from datasets/Catagory 0.csv\n",
      "âœ… Loaded 499 descriptions from datasets/Catagory 1.csv\n",
      "âœ… Loaded 499 descriptions from datasets/Catagory 2.csv\n",
      "âœ… Loaded 499 descriptions from datasets/Catagory 3.csv\n",
      "âœ… Loaded 262 descriptions from datasets/Catagory 4.csv\n",
      "âœ… Loaded 38 descriptions from datasets/Catagory 5.csv\n",
      "âœ… Loaded 69 descriptions from datasets/Catagory 6.csv\n",
      "\n",
      "ğŸ“‚ Scanning dataset folder: datasets\n",
      "â­ï¸ Skip Category 6\n",
      "\n",
      "âœ… Output complete: 2992 patients\n",
      "  Train: 2094 | Val: 300 | Test: 598\n",
      "ğŸ“ Output folder: datasets\n",
      "\n",
      "ğŸ“Š Dataset counts per class (stratified with rounding):\n",
      "  Category 0 (Total: 1195):\n",
      "    Train=836 (70.0%), Val=120 (10.0%), Test=239 (20.0%)\n",
      "  Category 1 (Total: 499):\n",
      "    Train=349 (69.9%), Val=50 (10.0%), Test=100 (20.0%)\n",
      "  Category 2 (Total: 500):\n",
      "    Train=350 (70.0%), Val=50 (10.0%), Test=100 (20.0%)\n",
      "  Category 3 (Total: 498):\n",
      "    Train=349 (70.1%), Val=50 (10.0%), Test=99 (19.9%)\n",
      "  Category 4 (Total: 261):\n",
      "    Train=183 (70.1%), Val=26 (10.0%), Test=52 (19.9%)\n",
      "  Category 5 (Total: 39):\n",
      "    Train=27 (69.2%), Val=4 (10.3%), Test=8 (20.5%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "\n",
    "VALID_VIEWS = ['L-CC', 'R-CC', 'L-MLO', 'R-MLO']\n",
    "\n",
    "def load_patient_descriptions(base_datasets_path):\n",
    "    descriptions = {}\n",
    "    for category_num in range(7):\n",
    "        csv_path = os.path.join(base_datasets_path, f\"Catagory {category_num}.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"âš ï¸ CSV not found: {csv_path}\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            for _, row in df.iterrows():\n",
    "                patient_id = str(row.iloc[0])\n",
    "                description = str(row.iloc[-1]) if len(row) > 1 else \"No description\"\n",
    "                descriptions[patient_id] = description\n",
    "            print(f\"âœ… Loaded {len(df)} descriptions from {csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to load {csv_path}: {e}\")\n",
    "    return descriptions\n",
    "\n",
    "\n",
    "def stratified_split(df, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    æ”¹é€²ç‰ˆåˆ†å±¤æŠ½æ¨£ï¼šä½¿ç”¨å››æ¨äº”å…¥é¿å…æˆªæ–·èª¤å·®\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    train_list, val_list, test_list = [], [], []\n",
    "\n",
    "    for label in sorted(df['label'].unique()):\n",
    "        df_label = df[df['label'] == label].sample(frac=1, random_state=seed)\n",
    "        n_total = len(df_label)\n",
    "        \n",
    "        # ä½¿ç”¨å››æ¨äº”å…¥è¨ˆç®—å„é›†åˆå¤§å°\n",
    "        n_train = max(1, round(n_total * train_ratio))\n",
    "        n_val = max(1, round(n_total * val_ratio))\n",
    "        \n",
    "        # ç¢ºä¿ç¸½æ•¸æ­£ç¢ºï¼štest é›†åˆå¸æ”¶æ‰€æœ‰èª¤å·®\n",
    "        n_test = max(1, n_total - n_train - n_val)\n",
    "        \n",
    "        # è™•ç†é‚Šç•Œæƒ…æ³ï¼šç¸½å’Œè¶…é n_total\n",
    "        if n_train + n_val + n_test > n_total:\n",
    "            excess = n_train + n_val + n_test - n_total\n",
    "            # å„ªå…ˆå¾æœ€å¤§çš„é›†åˆæ¸›å°‘\n",
    "            if n_train >= n_val and n_train > excess:\n",
    "                n_train -= excess\n",
    "            elif n_val > excess:\n",
    "                n_val -= excess\n",
    "            else:\n",
    "                n_test -= excess\n",
    "        \n",
    "        # è™•ç†é‚Šç•Œæƒ…æ³ï¼šç¸½å’Œå°æ–¼ n_totalï¼ˆç†è«–ä¸Šä¸æ‡‰ç™¼ç”Ÿï¼‰\n",
    "        elif n_train + n_val + n_test < n_total:\n",
    "            n_test += (n_total - (n_train + n_val + n_test))\n",
    "        \n",
    "        # ç¢ºä¿æ¯å€‹é›†åˆè‡³å°‘æœ‰ 1 ç­†è³‡æ–™ï¼ˆå¦‚æœé¡åˆ¥ç¸½æ•¸ >= 3ï¼‰\n",
    "        if n_total >= 3:\n",
    "            n_train = max(1, n_train)\n",
    "            n_val = max(1, n_val)\n",
    "            n_test = max(1, n_test)\n",
    "        \n",
    "        train_list.append(df_label.iloc[:n_train])\n",
    "        val_list.append(df_label.iloc[n_train:n_train+n_val])\n",
    "        test_list.append(df_label.iloc[n_train+n_val:n_train+n_val+n_test])\n",
    "\n",
    "    train_df = pd.concat(train_list).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    val_df = pd.concat(val_list).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_list).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def generate_multiview_csvs(base_dir, base_datasets_path, output_dir,\n",
    "                            train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, seed=42):\n",
    "    random.seed(seed)\n",
    "    patient_descriptions = load_patient_descriptions(base_datasets_path)\n",
    "    patients_data = []\n",
    "\n",
    "    print(f\"\\nğŸ“‚ Scanning dataset folder: {base_dir}\")\n",
    "    \n",
    "    for category in sorted(os.listdir(base_dir)):\n",
    "        category_path = os.path.join(base_dir, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        try:\n",
    "            label = int(category.replace(\"Category \", \"\"))\n",
    "            if label == 6:\n",
    "                print(f\"â­ï¸ Skip Category {label}\")\n",
    "                continue\n",
    "        except ValueError:\n",
    "            print(f\"âš ï¸ Skip unrecognized folder name '{category}'\")\n",
    "            continue\n",
    "\n",
    "        for patient_folder in os.listdir(category_path):\n",
    "            patient_path = os.path.join(category_path, patient_folder)\n",
    "            if not os.path.isdir(patient_path):\n",
    "                continue\n",
    "\n",
    "            patient_entry = {\n",
    "                'patient_id': f\"{category}/{patient_folder}\",\n",
    "                'label': label,\n",
    "                'description': patient_descriptions.get(patient_folder, \"No description available\")\n",
    "            }\n",
    "\n",
    "            for img_file in os.listdir(patient_path):\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    view_name = os.path.splitext(img_file)[0]\n",
    "                    if view_name in VALID_VIEWS:\n",
    "                        patient_entry[view_name] = os.path.join(category, patient_folder, img_file)\n",
    "\n",
    "            patients_data.append(patient_entry)\n",
    "\n",
    "    df = pd.DataFrame(patients_data)\n",
    "    df = df.reindex(columns=VALID_VIEWS + ['label', 'description', 'patient_id'])\n",
    "\n",
    "    missing_mask = df[VALID_VIEWS].isna().any(axis=1)\n",
    "    if missing_mask.any():\n",
    "        print(\"\\nâš ï¸ Patients missing some views:\")\n",
    "        for _, row in df[missing_mask].iterrows():\n",
    "            missing_views = [v for v in VALID_VIEWS if pd.isna(row[v])]\n",
    "            print(f\"  - {row['patient_id']} missing {', '.join(missing_views)}\")\n",
    "\n",
    "    df = df.dropna(subset=VALID_VIEWS)\n",
    "\n",
    "    # åˆ†å±¤æŠ½æ¨£ï¼ˆä½¿ç”¨æ”¹é€²ç‰ˆï¼‰\n",
    "    train_df, val_df, test_df = stratified_split(df, train_ratio, val_ratio, test_ratio, seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_df.to_csv(os.path.join(output_dir, \"train_labels.csv\"), index=False)\n",
    "    val_df.to_csv(os.path.join(output_dir, \"val_labels.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(output_dir, \"test_labels.csv\"), index=False)\n",
    "\n",
    "    print(f\"\\nâœ… Output complete: {len(df)} patients\")\n",
    "    print(f\"  Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "    print(f\"ğŸ“ Output folder: {output_dir}\")\n",
    "\n",
    "    # æ¯å€‹é¡åˆ¥çµ±è¨ˆï¼ˆåŒ…å«å¯¦éš›æ¯”ä¾‹ï¼‰\n",
    "    print(\"\\nğŸ“Š Dataset counts per class (stratified with rounding):\")\n",
    "    for label in sorted(df['label'].unique()):\n",
    "        n_total_c = (df['label'] == label).sum()\n",
    "        n_train_c = (train_df['label'] == label).sum()\n",
    "        n_val_c   = (val_df['label'] == label).sum()\n",
    "        n_test_c  = (test_df['label'] == label).sum()\n",
    "        \n",
    "        actual_train_ratio = n_train_c / n_total_c if n_total_c > 0 else 0\n",
    "        actual_val_ratio = n_val_c / n_total_c if n_total_c > 0 else 0\n",
    "        actual_test_ratio = n_test_c / n_total_c if n_total_c > 0 else 0\n",
    "        \n",
    "        print(f\"  Category {label} (Total: {n_total_c}):\")\n",
    "        print(f\"    Train={n_train_c} ({actual_train_ratio:.1%}), \"\n",
    "              f\"Val={n_val_c} ({actual_val_ratio:.1%}), \"\n",
    "              f\"Test={n_test_c} ({actual_test_ratio:.1%})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"datasets\"\n",
    "    base_datasets_path = base_dir\n",
    "    output_dir = base_dir\n",
    "\n",
    "    generate_multiview_csvs(\n",
    "        base_dir=base_dir,\n",
    "        base_datasets_path=base_datasets_path,\n",
    "        output_dir=output_dir,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.2,\n",
    "        seed=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7bf291",
   "metadata": {},
   "source": [
    "# clahe \n",
    "# crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c69b19a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹å½±åƒå°ºå¯¸: (2294, 1914)\n",
      "è£åˆ‡å¾Œå½±åƒå°ºå¯¸: (2142, 793)\n",
      "æœ‰æ•ˆåƒç´ æ•¸: 835177\n",
      "é®ç½©è¦†è“‹ç‡: 19.02%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def preprocess_and_autocrop(image_path, threshold_offset=-50, padding_percent=0.08, min_area_ratio=0.01):\n",
    "    \"\"\"\n",
    "    é è™•ç†ä¸¦è‡ªå‹•è£åˆ‡ä¹³æˆ¿Xå…‰å½±åƒ\n",
    "    \n",
    "    åƒæ•¸:\n",
    "        image_path: è¼¸å…¥å½±åƒè·¯å¾‘\n",
    "        threshold_offset: é–¾å€¼åç§»é‡ï¼ˆè² å€¼=æ›´å¯¬é¬†ï¼Œé è¨­-20ï¼‰\n",
    "        padding_percent: è£åˆ‡é‚Šè·ç™¾åˆ†æ¯”ï¼ˆé è¨­0.05=5%ï¼‰\n",
    "        min_area_ratio: æœ€å°è¼ªå»“é¢ç©æ¯”ä¾‹ï¼ˆé è¨­0.02=2%ï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "        original: åŸå§‹ç°éšå½±åƒ\n",
    "        enhanced: CLAHE å¢å¼·å¾Œçš„å½±åƒ\n",
    "        mask: åµæ¸¬åˆ°çš„ä¹³æˆ¿é®ç½©\n",
    "        cropped: è‡ªå‹•è£åˆ‡å¾Œçš„å½±åƒ\n",
    "    \"\"\"\n",
    "    \n",
    "    # è®€å–å½±åƒ\n",
    "    original = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if original is None:\n",
    "        raise ValueError(f\"ç„¡æ³•è®€å–å½±åƒ: {image_path}\")\n",
    "    \n",
    "    # 1. CLAHE å°æ¯”åº¦å¢å¼·\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(original)\n",
    "    \n",
    "    # 2. å½±åƒé è™•ç†\n",
    "    blurred = cv2.GaussianBlur(enhanced, (5, 5), 0)\n",
    "    \n",
    "    # 3. äºŒå€¼åŒ– - ä½¿ç”¨æ›´ä½çš„é–¾å€¼ä¾†åŒ…å«æš—å€ï¼ˆå¦‚ä¹³é ­ï¼‰\n",
    "    # å…ˆç”¨ Otsu å¾—åˆ°åŸºæº–é–¾å€¼\n",
    "    otsu_thresh, _ = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    # å¤§å¹…é™ä½é–¾å€¼ä»¥åŒ…å«ä¹³é ­ç­‰æš—å€\n",
    "    lower_thresh = max(5, otsu_thresh + threshold_offset)\n",
    "    _, binary = cv2.threshold(blurred, lower_thresh, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # 4. å½¢æ…‹å­¸è™•ç† - æ›´æ¿€é€²çš„é–‰é‹ç®—\n",
    "    kernel_large = np.ones((11, 11), np.uint8)\n",
    "    kernel_small = np.ones((5, 5), np.uint8)\n",
    "    \n",
    "    # æ›´å¤šæ¬¡é–‰é‹ç®—ï¼Œå¡«è£œæ›´å¤šå€åŸŸ\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel_large, iterations=5)\n",
    "    # æ¸›å°‘é–‹é‹ç®—æ¬¡æ•¸ï¼Œä¿ç•™æ›´å¤šå€åŸŸ\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_small, iterations=1)\n",
    "    \n",
    "    # 5. å°‹æ‰¾è¼ªå»“\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # 6. éæ¿¾ä¸¦é¸æ“‡æœ€å¤§è¼ªå»“ - é™ä½æœ€å°é¢ç©è¦æ±‚\n",
    "    min_area = original.shape[0] * original.shape[1] * min_area_ratio\n",
    "    valid_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]\n",
    "    \n",
    "    if len(valid_contours) == 0:\n",
    "        raise ValueError(\"æœªæ‰¾åˆ°æœ‰æ•ˆè¼ªå»“\")\n",
    "    \n",
    "    largest_contour = max(valid_contours, key=cv2.contourArea)\n",
    "    \n",
    "    # 7. è¨ˆç®—å‡¸åŒ…ï¼ŒåŒ…å«æ›´å®Œæ•´çš„å€åŸŸ\n",
    "    hull = cv2.convexHull(largest_contour)\n",
    "    \n",
    "    # 8. å‰µå»ºé®ç½© - ä½¿ç”¨å‡¸åŒ…ç¢ºä¿åŒ…å«å®Œæ•´ä¹³æˆ¿å€åŸŸ\n",
    "    mask = np.zeros_like(original)\n",
    "    cv2.drawContours(mask, [hull], -1, 255, thickness=cv2.FILLED)\n",
    "    \n",
    "    # 9. é¡å¤–è™•ç†ï¼šå¡«è£œå…§éƒ¨å­”æ´ï¼ˆå¦‚ä¹³é ­å€åŸŸï¼‰\n",
    "    # å°‹æ‰¾æ‰€æœ‰å…§éƒ¨è¼ªå»“\n",
    "    contours_all, hierarchy = cv2.findContours(mask.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # å¡«è£œæ‰€æœ‰å…§éƒ¨å­”æ´\n",
    "    for i in range(len(contours_all)):\n",
    "        if hierarchy[0][i][3] != -1:  # å¦‚æœæœ‰çˆ¶è¼ªå»“ï¼ˆå³å…§éƒ¨å­”æ´ï¼‰\n",
    "            cv2.drawContours(mask, contours_all, i, 255, thickness=cv2.FILLED)\n",
    "    \n",
    "    # 10. æ“´å¼µé®ç½©é‚Šç·£ï¼ˆæ›´æº«å’Œçš„æ“´å¼µä»¥ä¿ç•™ç´°ç¯€ï¼‰\n",
    "    kernel_dilate = np.ones((15, 15), np.uint8)\n",
    "    mask = cv2.dilate(mask, kernel_dilate, iterations=3)\n",
    "    \n",
    "    # 10. ç¾½åŒ–é‚Šç·£\n",
    "    mask_blurred = cv2.GaussianBlur(mask, (21, 21), 0)\n",
    "    \n",
    "    # 11. ç²å–é‚Šç•Œæ¡†é€²è¡Œè£åˆ‡\n",
    "    x, y, w, h = cv2.boundingRect(hull)\n",
    "    \n",
    "    # æ·»åŠ æ›´å¤§çš„é‚Šè·ï¼ˆåŸºæ–¼å½±åƒå°ºå¯¸çš„ç™¾åˆ†æ¯”ï¼‰\n",
    "    padding_x = int(original.shape[1] * padding_percent)\n",
    "    padding_y = int(original.shape[0] * padding_percent)\n",
    "    \n",
    "    x = max(0, x - padding_x)\n",
    "    y = max(0, y - padding_y)\n",
    "    w = min(original.shape[1] - x, w + 2 * padding_x)\n",
    "    h = min(original.shape[0] - y, h + 2 * padding_y)\n",
    "    \n",
    "    # 12. è£åˆ‡å½±åƒå’Œé®ç½©\n",
    "    cropped = enhanced[y:y+h, x:x+w]\n",
    "\n",
    "    return original, enhanced, mask, cropped\n",
    "\n",
    "\n",
    "def display_results(original, enhanced, mask, cropped):\n",
    "    \"\"\"\n",
    "    é¡¯ç¤ºè™•ç†çµæœ\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(original, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.title(\"After CLAHE\")\n",
    "    plt.imshow(enhanced, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.title(\"Detected Mask\")\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.title(\"Auto-cropped Image\")\n",
    "    plt.imshow(cropped, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === ä½¿ç”¨ç¯„ä¾‹ ===\n",
    "if __name__ == \"__main__\":\n",
    "    # === æ¸¬è©¦ ===\n",
    "    image_path = \"datasets/Category 0/XA2016010003906/R-CC.jpg\"\n",
    "    \n",
    "    try:    \n",
    "        original, enhanced, mask, cropped = preprocess_and_autocrop(\n",
    "            image_path,\n",
    "            threshold_offset=-50,    # æ›´å¯¬é¬†çš„é–¾å€¼ï¼ˆé è¨­-20ï¼‰\n",
    "            padding_percent=0.08,    # æ›´å¤§çš„é‚Šè·ï¼ˆé è¨­0.05=5%ï¼‰\n",
    "            min_area_ratio=0.01      # æ›´ä½çš„æœ€å°é¢ç©ï¼ˆé è¨­0.02=2%ï¼‰\n",
    "        )\n",
    "        \n",
    "        #display_results(original, enhanced, mask, cropped)\n",
    "        \n",
    "        # è¼¸å‡ºçµ±è¨ˆè³‡è¨Š\n",
    "        print(f\"åŸå§‹å½±åƒå°ºå¯¸: {original.shape}\")\n",
    "        print(f\"è£åˆ‡å¾Œå½±åƒå°ºå¯¸: {cropped.shape}\")\n",
    "        print(f\"æœ‰æ•ˆåƒç´ æ•¸: {np.sum(mask > 0)}\")\n",
    "        print(f\"é®ç½©è¦†è“‹ç‡: {np.sum(mask > 0) / (mask.shape[0] * mask.shape[1]) * 100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"éŒ¯èª¤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df367bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def save_cropped_images(cropped, image_path, output_root):\n",
    "    \"\"\"\n",
    "    ä¿å­˜è£åˆ‡å¾Œçš„å½±åƒï¼Œä¸¦ç¶­æŒèˆ‡åŸå§‹è³‡æ–™å¤¾ç›¸åŒçš„çµæ§‹ã€‚\n",
    "    \"\"\"\n",
    "    relative_path = os.path.relpath(image_path, start=\"datasets\")\n",
    "    output_dir = os.path.join(output_root, os.path.dirname(relative_path))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    cv2.imwrite(os.path.join(output_dir, f\"{base_name}.jpg\"), cropped)\n",
    "\n",
    "def process_dataset(input_root, output_root):\n",
    "    \"\"\"\n",
    "    éæ­·è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰å½±åƒï¼Œé€²è¡Œé è™•ç†ä¸¦ä¿å­˜çµæœã€‚\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(input_root):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tiff')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    original, enhanced, mask, cropped = preprocess_and_autocrop(image_path)\n",
    "                    save_cropped_images(cropped, image_path, output_root)\n",
    "                    #print(f\"è™•ç†å®Œæˆ: {image_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"è™•ç†å¤±æ•—: {image_path}, éŒ¯èª¤: {e}\")\n",
    "\n",
    "# === ä¸»ç¨‹å¼ ===\n",
    "if __name__ == \"__main__\":\n",
    "    input_root = \"datasets\"  # åŸå§‹è³‡æ–™å¤¾\n",
    "    output_root = \"cropped_datasets\"  # é è™•ç†å¾Œçš„è³‡æ–™å¤¾\n",
    "    process_dataset(input_root, output_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pacs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
